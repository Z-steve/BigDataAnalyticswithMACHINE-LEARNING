ğŸ“‚ Found 8 CSV files. Loading...

âœ… Dataset loaded successfully!
Shape: (2830743, 79)
   Destination Port  Flow Duration  ...  Idle Min   Label
0             49188              4  ...         0  BENIGN
1             49188              1  ...         0  BENIGN
2             49188              1  ...         0  BENIGN
3             49188              1  ...         0  BENIGN
4             49486              3  ...         0  BENIGN

[5 rows x 79 columns]

âœ… Removed 0 entries that were not DDoS, BENIGN, DoS GoldenEye, DoS Hulk, DoS Slowhttptest, DoS slowloris, Botnet, FTP-Patator, SSH-Patator, Heartbleed, Infiltration, PortScan, Web Attack ï¿½ Brute Force, Web Attack ï¿½ Sql Injection, Web Attack ï¿½ XSS

âœ… Data Preprocessing Completato! Struttura finale: (2520798, 79)
âœ… Mapping delle Labels eseguito con successo!
Label
BENIGN            2095057
DOS                321759
RECONNAISSANCE      90694
BRUTE_FORCE          9150
WEB_ATTACK           2190
BOTNET               1948
Name: count, dtype: int64

âœ… Extracted features (numerical only): (2520798, 78), Labels: (2520798,)
âœ… Dati partizionati: Dimensione del set per il train: (1512478, 78), Dimensione del set per il test: (1008320, 78)
âœ… Reduced BENIGN to 188555 entries
New class distribution:
Label
DOS               193055
BENIGN            188555
RECONNAISSANCE     54416
BRUTE_FORCE         5490
WEB_ATTACK          1314
BOTNET              1169
Name: count, dtype: int64

âœ… Dati pronti!
Forma di X_train: (443999, 78)
Forma di y_train: (443999,)
Forma di X_test: (1008320, 78)
Forma di y_test: (1008320,)

ğŸŒ² Training Random Forest...

ğŸ” Training Random_forest Model...
ğŸš€ Current model parameters before training: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
âœ… Training time: 37.57 seconds

ğŸ† Evaluating Model Performance...
âœ… Prediction time: 2.87 seconds

ğŸ“Š Model Performance:
                precision    recall  f1-score   support

        BENIGN     0.9998    0.9987    0.9993    838023
        BOTNET     0.7024    0.9089    0.7924       779
   BRUTE_FORCE     0.9997    0.9964    0.9981      3660
           DOS     0.9969    0.9995    0.9982    128704
RECONNAISSANCE     0.9897    0.9993    0.9944     36278
    WEB_ATTACK     0.9838    0.9692    0.9764       876

      accuracy                         0.9987   1008320
     macro avg     0.9454    0.9787    0.9598   1008320
  weighted avg     0.9988    0.9987    0.9988   1008320


ğŸ”² Confusion Matrix:

âœ… Training Accuracy: 0.9992
âœ… Testing Accuracy: 0.9987

ğŸ“Š Plotting Feature Importance for Random Forest...

ğŸš€ Training XGBoost...

ğŸ” Training Xgboost Model...
ğŸš€ Current model parameters before training: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 1.0, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'mlogloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 1.0, 'tree_method': None, 'validate_parameters': None, 'verbosity': 1, 'use_label_encoder': False}

  warnings.warn(smsg, UserWarning)
âœ… Training time: 54.46 seconds

ğŸ” Evaluating XGBoost...

ğŸ† Evaluating Model Performance...
âœ… Prediction time: 6.31 seconds

ğŸ“Š Model Performance:
              precision    recall  f1-score   support

           0     0.9999    0.9989    0.9994    838023
           1     0.6872    0.9615    0.8015       779
           2     0.9978    0.9995    0.9986      3660
           3     0.9982    0.9998    0.9990    128704
           4     0.9895    0.9995    0.9945     36278
           5     0.9896    0.9817    0.9857       876

    accuracy                         0.9990   1008320
   macro avg     0.9437    0.9901    0.9631   1008320
weighted avg     0.9991    0.9990    0.9990   1008320


ğŸ”² Confusion Matrix:

âœ… Training Accuracy: 0.9996
âœ… Testing Accuracy: 0.9990

ğŸ“Š Plotting Feature Importance for XGBoost...

ğŸŒŸ Training LightGBM...

ğŸ” Training Lightgbm Model...
ğŸš€ Current model parameters before training: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.5, 'importance_type': 'split', 'learning_rate': 0.01, 'max_depth': 7, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'multiclass', 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.5, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'metric': 'multi_logloss', 'verbose': -1}
âœ… Training time: 91.78 seconds

ğŸ” Evaluating LightGBM...

ğŸ† Evaluating Model Performance...
âœ… Prediction time: 44.60 seconds

ğŸ“Š Model Performance:
              precision    recall  f1-score   support

           0     0.9999    0.9986    0.9992    838023
           1     0.6928    0.9756    0.8102       779
           2     0.9992    0.9992    0.9992      3660
           3     0.9963    0.9997    0.9980    128704
           4     0.9896    0.9994    0.9945     36278
           5     0.9873    0.9783    0.9828       876

    accuracy                         0.9987   1008320
   macro avg     0.9442    0.9918    0.9640   1008320
weighted avg     0.9988    0.9987    0.9988   1008320


ğŸ”² Confusion Matrix:

âœ… Training Accuracy: 0.9993
âœ… Testing Accuracy: 0.9987

ğŸ“Š Plotting Feature Importance for LightGBM...

ğŸ“Š Comparing Model Performance...

âœ… All models trained, evaluated, and visualized!

Process finished with exit code 0
